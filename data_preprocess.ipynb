{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7f91e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_dialog_dataset(raw_data_path):\n",
    "\n",
    "    with open(raw_data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    qa_examples = []\n",
    "\n",
    "    for dialogue_id, dialogues in data.items():\n",
    "        for conversation in dialogues:\n",
    "            conversation_history = []\n",
    "            for utt in conversation:\n",
    "                conversation_history.append(utt[\"utterance\"])#(f'{utt[\"speaker\"]}: {utt[\"utterance\"]}')\n",
    "                if utt[\"emotion\"] != \"neutral\":\n",
    "                    context = \" \".join(conversation_history)\n",
    "                    target = utt[\"utterance\"]\n",
    "                    emotion = utt[\"emotion\"]\n",
    "                    question = f\"The target utterance is '{target}'. What is the causal span that triggers the emotion {emotion}?\"\n",
    "                    causal_spans = utt.get(\"expanded emotion cause span\", [])\n",
    "                    if causal_spans:\n",
    "                        answer_text = causal_spans[0]\n",
    "                    else:\n",
    "                        answer_text = \"\"\n",
    "\n",
    "                    answer_start = context.find(answer_text)\n",
    "                    if answer_start == -1:\n",
    "                        answer_start = 0\n",
    "\n",
    "                    evidence_turns = utt.get(\"expanded emotion cause evidence\", [])\n",
    "                    if evidence_turns:\n",
    "                        evidence_val = evidence_turns[0]\n",
    "                        try:\n",
    "                            evidence_turn = int(evidence_val)\n",
    "                        except ValueError:\n",
    "                            # If conversion fails, you can decide to either skip or assign a default value.\n",
    "                            evidence_turn = None\n",
    "\n",
    "                    evidence_utterance = None\n",
    "                    if evidence_turn and evidence_turn - 1 < len(conversation_history):\n",
    "                        evidence_utterance = conversation_history[evidence_turn - 1]\n",
    "\n",
    "                    qa_example = {\n",
    "                        \"dialogue_id\": dialogue_id,\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"answer\": answer_text,\n",
    "                        \"answer_start\": answer_start,\n",
    "                        \"evidence_turn\": evidence_turn,\n",
    "                        \"evidence_utterance\": evidence_utterance\n",
    "                    }\n",
    "\n",
    "                    qa_examples.append(qa_example)\n",
    "                    \n",
    "    return qa_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed6f7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_examples(example, tokenizer, max_length=512):\n",
    "    try:\n",
    "        # Tokenize context and question with offset mapping.\n",
    "        inputs = tokenizer(\n",
    "            example[\"context\"],\n",
    "            example[\"question\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        # Retrieve and remove the offset mapping.\n",
    "        offsets = inputs.pop(\"offset_mapping\")\n",
    "        \n",
    "        answer_text = example[\"answer\"]\n",
    "        answer_start = example[\"answer_start\"]\n",
    "\n",
    "        start_position, end_position = None, None\n",
    "        # Find token indices corresponding to the answer span.\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= answer_start < end:\n",
    "                start_position = idx\n",
    "            if start < answer_start + len(answer_text) <= end:\n",
    "                end_position = idx\n",
    "                break\n",
    "\n",
    "        if start_position is None:\n",
    "            print(f\"[WARNING] Start position not found for answer: '{answer_text}' in context.\")\n",
    "            start_position = 0\n",
    "        if end_position is None:\n",
    "            print(f\"[WARNING] End position not found for answer: '{answer_text}' in context.\")\n",
    "            end_position = 0\n",
    "\n",
    "        inputs[\"start_positions\"] = start_position\n",
    "        inputs[\"end_positions\"] = end_position\n",
    "        return inputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Problem with example:\\n{example}\\nError: {e}\")\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f7b1b-666a-4fe6-8056-0a1a72757083",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a60bcbd3-f5ca-45ce-84a9-cb01e18f0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_create_dataset(raw_data_path):\n",
    "    qa_examples = create_dialog_dataset(raw_data_path)\n",
    "    df = pd.DataFrame(qa_examples)\n",
    "    return Dataset.from_dict(df)\n",
    "\n",
    "# Define file paths (adjust according to your folder structure)\n",
    "train_data_path = \"data/original_annotation/dailydialog_train.json\"\n",
    "val_data_path = \"data/original_annotation/dailydialog_valid.json\"\n",
    "test_data_path = \"data/original_annotation/dailydialog_test.json\"\n",
    "\n",
    "# Load raw datasets.\n",
    "train_dataset = load_and_create_dataset(train_data_path)\n",
    "val_dataset   = load_and_create_dataset(val_data_path)\n",
    "test_dataset  = load_and_create_dataset(test_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab982450-f036-4b09-8022-2daff62a0856",
   "metadata": {},
   "source": [
    "## Pre-process and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef2d179f-2210-465c-8c72-1ef03f8233d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set with SpanBERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ SpanBERT:  55%|█████████████████████                 | 2522/4562 [00:01<00:01, 1367.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] End position not found for answer: 'I think I want to order the new Accord then . It looks like an excellent car .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ SpanBERT:  71%|███████████████████████████           | 3245/4562 [00:02<00:01, 1219.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Start position not found for answer: 'if you can cook things this good .' in context.\n",
      "[WARNING] End position not found for answer: 'if you can cook things this good .' in context.\n",
      "[WARNING] Start position not found for answer: 'I'm a great cook' in context.\n",
      "[WARNING] End position not found for answer: 'I'm a great cook' in context.\n",
      "[WARNING] End position not found for answer: 'I should ask some of my friends for advice before I make a decision . I'm sorry .' in context.\n",
      "[WARNING] Start position not found for answer: 'thanks for your advice .' in context.\n",
      "[WARNING] End position not found for answer: 'thanks for your advice .' in context.\n",
      "[WARNING] Start position not found for answer: 'thanks for your advice . I will probably come back later .' in context.\n",
      "[WARNING] End position not found for answer: 'thanks for your advice . I will probably come back later .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ SpanBERT:  99%|█████████████████████████████████████▋| 4529/4562 [00:03<00:00, 1367.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Start position not found for answer: 'I have a lot to learn from you .' in context.\n",
      "[WARNING] End position not found for answer: 'I have a lot to learn from you .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ SpanBERT: 100%|██████████████████████████████████████| 4562/4562 [00:03<00:00, 1340.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation set with SpanBERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Val @ SpanBERT: 100%|██████████████████████████████████████████| 200/200 [00:00<00:00, 1275.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test set with SpanBERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Test @ SpanBERT:   0%|                                                    | 0/1099 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Start position not found for answer: 'There it is . That is the pattern ! The set behind you .' in context.\n",
      "[WARNING] End position not found for answer: 'There it is . That is the pattern ! The set behind you .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Test @ SpanBERT: 100%|███████████████████████████████████████| 1099/1099 [00:00<00:00, 1300.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set with RoBERTa tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ RoBERTa:  78%|██████████████████████████████▌        | 3581/4562 [00:02<00:00, 1611.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] End position not found for answer: 'thanks for your advice .' in context.\n",
      "[WARNING] Start position not found for answer: 'thanks for your advice . I will probably come back later .' in context.\n",
      "[WARNING] End position not found for answer: 'thanks for your advice . I will probably come back later .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ RoBERTa:  96%|█████████████████████████████████████▍ | 4381/4562 [00:02<00:00, 1482.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Start position not found for answer: 'I have a lot to learn from you .' in context.\n",
      "[WARNING] End position not found for answer: 'I have a lot to learn from you .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train @ RoBERTa: 100%|███████████████████████████████████████| 4562/4562 [00:02<00:00, 1577.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation set with RoBERTa tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Val @ RoBERTa: 100%|███████████████████████████████████████████| 200/200 [00:00<00:00, 1499.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test set with RoBERTa tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Test @ RoBERTa:   0%|                                                     | 0/1099 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] End position not found for answer: 'There it is . That is the pattern ! The set behind you .' in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Test @ RoBERTa: 100%|████████████████████████████████████████| 1099/1099 [00:00<00:00, 1524.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████| 4562/4562 [00:00<00:00, 17653.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████| 200/200 [00:00<00:00, 5042.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████| 1099/1099 [00:00<00:00, 13757.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████| 4562/4562 [00:00<00:00, 20597.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████| 200/200 [00:00<00:00, 7739.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████| 1099/1099 [00:00<00:00, 11472.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed datasets:\n",
      " - SpanBERT: train, val, test saved as 'spanbert_preprocessed_*_dataset'\n",
      " - RoBERTa: train, val, test saved as 'roberta_preprocessed_*_dataset'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizers for SpanBERT and RoBERTa.\n",
    "tokenizer_spanbert = AutoTokenizer.from_pretrained(\"SpanBert/spanbert-base-cased\")\n",
    "tokenizer_roberta  = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Preprocess (tokenize) each dataset for each model.\n",
    "print(\"Preprocessing training set with SpanBERT tokenizer...\")\n",
    "tokenized_train_spanbert = train_dataset.map(lambda x: preprocess_examples(x, tokenizer_spanbert),\n",
    "                                               batched=False, \n",
    "                                               desc=\"Preprocessing Train @ SpanBERT\")\n",
    "print(\"Preprocessing validation set with SpanBERT tokenizer...\")\n",
    "tokenized_val_spanbert = val_dataset.map(lambda x: preprocess_examples(x, tokenizer_spanbert),\n",
    "                                           batched=False, \n",
    "                                           desc=\"Preprocessing Val @ SpanBERT\")\n",
    "print(\"Preprocessing test set with SpanBERT tokenizer...\")\n",
    "tokenized_test_spanbert = test_dataset.map(lambda x: preprocess_examples(x, tokenizer_spanbert),\n",
    "                                             batched=False, \n",
    "                                             desc=\"Preprocessing Test @ SpanBERT\")\n",
    "\n",
    "print(\"Preprocessing training set with RoBERTa tokenizer...\")\n",
    "tokenized_train_roberta = train_dataset.map(lambda x: preprocess_examples(x, tokenizer_roberta),\n",
    "                                              batched=False, \n",
    "                                              desc=\"Preprocessing Train @ RoBERTa\")\n",
    "print(\"Preprocessing validation set with RoBERTa tokenizer...\")\n",
    "tokenized_val_roberta = val_dataset.map(lambda x: preprocess_examples(x, tokenizer_roberta),\n",
    "                                          batched=False, \n",
    "                                          desc=\"Preprocessing Val @ RoBERTa\")\n",
    "print(\"Preprocessing test set with RoBERTa tokenizer...\")\n",
    "tokenized_test_roberta = test_dataset.map(lambda x: preprocess_examples(x, tokenizer_roberta),\n",
    "                                            batched=False, \n",
    "                                            desc=\"Preprocessing Test @ RoBERTa\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Save Preprocessed Datasets to Disk\n",
    "# -------------------------\n",
    "tokenized_train_spanbert.save_to_disk(\"SpanBert/spanbert_preprocessed_train_dataset\")\n",
    "tokenized_val_spanbert.save_to_disk(\"SpanBert/spanbert_preprocessed_val_dataset\")\n",
    "tokenized_test_spanbert.save_to_disk(\"SpanBert/spanbert_preprocessed_test_dataset\")\n",
    "\n",
    "tokenized_train_roberta.save_to_disk(\"Roberta/roberta_preprocessed_train_dataset\")\n",
    "tokenized_val_roberta.save_to_disk(\"Roberta/roberta_preprocessed_val_dataset\")\n",
    "tokenized_test_roberta.save_to_disk(\"Roberta/roberta_preprocessed_test_dataset\")\n",
    "\n",
    "print(\"Saved preprocessed datasets:\")\n",
    "print(\" - SpanBERT: train, val, test saved as 'spanbert_preprocessed_*_dataset'\")\n",
    "print(\" - RoBERTa: train, val, test saved as 'roberta_preprocessed_*_dataset'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d4e5f-6f55-4a7a-8c2b-cd4e99155480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
