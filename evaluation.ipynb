{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "\n",
    "\n",
    "tokenizer_spanbert = AutoTokenizer.from_pretrained(\"SpanBert/spanbert-base-cased\")\n",
    "tokenizer_roberta  = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model_directory = \"spanbert_qa_finetuned_model\"\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_directory)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model is not empty: \", model is not None)\n",
    "#print(\"Tokenizer is not empty: \", tokenizer is not None)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset_roberta = load_from_disk(\"Roberta/roberta_preprocessed_train_dataset/data-00000-of-00001.arrow\")\n",
    "val_dataset_roberta = load_from_disk(\"Roberta/roberta_preprocessed_val_dataset/data-00000-of-00001.arrow\")\n",
    "test_dataset_roberta = load_from_disk(\"Roberta/roberta_preprocessed_test_dataset/data-00000-of-00001.arrow\")"
   ],
   "id": "a694a734f97906bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset_spanbert = load_from_disk(\"SpanBert/spanbert_preprocessed_train_dataset/data-00000-of-00001.arrow\")\n",
    "val_dataset_spanbert = load_from_disk(\"SpanBert/spanbert_preprocessed_train_dataset/data-00000-of-00001.arrow\")\n",
    "test_dataset_spanbert = load_from_disk(\"SpanBert/spanbert_preprocessed_train_dataset/data-00000-of-00001.arrow\")"
   ],
   "id": "f5f43d2521c92615"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(example, model, tokenizer, device):\n",
    "\n",
    "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0)  # Shape: [1, seq_length]\n",
    "    attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0)\n",
    "\n",
    "    inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    if \"token_type_ids\" in example:\n",
    "        token_type_ids = torch.tensor(example[\"token_type_ids\"]).unsqueeze(0)\n",
    "        inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().numpy()[0]\n",
    "    end_logits = outputs.end_logits.cpu().numpy()[0]\n",
    "\n",
    "    start_index = np.argmax(start_logits)\n",
    "    end_index = np.argmax(end_logits)\n",
    "\n",
    "    # Extract predicted token IDs from the original input_ids list\n",
    "    predicted_token_ids = example[\"input_ids\"][start_index : end_index + 1]\n",
    "\n",
    "    # Decode token IDs to text using the fallback tokenizer\n",
    "    predicted_answer = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    return predicted_answer"
   ],
   "id": "27f1e5dd27e1724c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluation(predicted_answer, reference_answer, dataset, model, tokenizer, device):\n",
    "    predicted_answers = []\n",
    "    reference_answers = []\n",
    "\n",
    "    for example in dataset:\n",
    "        pred_ans = predict(example, model, tokenizer, device)\n",
    "        predicted_answers.append(pred_ans)\n",
    "        reference_answers.append(example[\"answer\"])\n",
    "\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    #f1_metric = evaluate.load(\"f1\")\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "    acc = accuracy_metric.compute(predictions=predicted_answers, references=reference_answers)\n",
    "\n",
    "    # Compute Macro F1 Score.\n",
    "    #f1 = f1_metric.compute(predictions=predicted_answers, references=reference_answers, average=\"macro\")\n",
    "\n",
    "    # Compute BLEU Score.\n",
    "    # BLEU requires tokenized input (lists of tokens).\n",
    "    bleu = bleu_metric.compute(\n",
    "        predictions=[pred.split() for pred in predicted_answers],\n",
    "        references=[[ref.split()] for ref in reference_answers]\n",
    "    )\n",
    "\n",
    "    return acc, bleu, predicted_answers, reference_answers\n",
    "\n"
   ],
   "id": "239c92be472a6bd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_exact_match(pred, ref):\n",
    "    # Exact match: 1 if they are identical, else 0.\n",
    "    return int(pred.strip() == ref.strip())\n",
    "\n",
    "def compute_f1(pred, ref):\n",
    "    # Tokenize the answers.\n",
    "    pred_tokens = pred.split()\n",
    "    ref_tokens = ref.split()\n",
    "\n",
    "    # Handle cases where both are empty.\n",
    "    if len(pred_tokens) == 0 and len(ref_tokens) == 0:\n",
    "        return 1.0\n",
    "    # If one is empty and the other is not, f1 is zero.\n",
    "    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counter = Counter(pred_tokens)\n",
    "    ref_counter = Counter(ref_tokens)\n",
    "    common = pred_counter & ref_counter\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(ref_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def eval_reccon_metrics(predicted_answers, reference_answers):\n",
    "    em_all = []\n",
    "    f1_all = []\n",
    "    em_positive = []  # for examples with non-empty reference answer\n",
    "    f1_positive = []\n",
    "    em_negative = []  # for examples where reference answer is empty\n",
    "    f1_negative = []\n",
    "\n",
    "    for pred, ref in zip(predicted_answers, reference_answers):\n",
    "        em = compute_exact_match(pred, ref)\n",
    "        f1_val = compute_f1(pred, ref)\n",
    "        em_all.append(em)\n",
    "        f1_all.append(f1_val)\n",
    "\n",
    "        if ref == \"\":  # Negative example (expected prediction: empty)\n",
    "            em_negative.append(em)\n",
    "            f1_negative.append(f1_val)\n",
    "        else:\n",
    "            em_positive.append(em)\n",
    "            f1_positive.append(f1_val)\n",
    "\n",
    "    overall_em = np.mean(em_all)\n",
    "    overall_f1 = np.mean(f1_all)\n",
    "    positive_em = np.mean(em_positive) if em_positive else 0\n",
    "    positive_f1 = np.mean(f1_positive) if f1_positive else 0\n",
    "    negative_em = np.mean(em_negative) if em_negative else 0\n",
    "    negative_f1 = np.mean(f1_negative) if f1_negative else 0\n",
    "\n",
    "    result = {\n",
    "        \"overall_em\": overall_em,\n",
    "        \"overall_f1\": overall_f1,\n",
    "        \"positive_em\": positive_em,\n",
    "        \"positive_f1\": positive_f1,\n",
    "        \"negative_em\": negative_em,\n",
    "        \"negative_f1\": negative_f1,\n",
    "    }\n",
    "    return result\n",
    "\n"
   ],
   "id": "9871dce0b1b45993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "result =\n",
    "\n",
    "print(\"Overall Exact Match (EM):\", result[\"overall_em\"])\n",
    "print(\"Overall F1 Score:\", result[\"overall_f1\"])\n",
    "print(\"Positive Exact Match (EM):\", result[\"positive_em\"])\n",
    "print(\"Positive F1 Score:\", result[\"positive_f1\"])\n",
    "print(\"Negative Exact Match (EM):\", result[\"negative_em\"])\n",
    "print(\"Negative F1 Score:\", result[\"negative_f1\"])"
   ],
   "id": "d228294c12704032"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
